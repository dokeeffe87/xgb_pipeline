{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A custom xgboost classifier pipeline which does early stopping within cross validation, feature selection, and probability calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.transforms as mtransforms\n",
    "from matplotlib import markers\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, auc, classification_report, f1_score, log_loss, precision_recall_curve, roc_curve, roc_auc_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, clone\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectFromModel, SelectKBest\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "from random import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot style\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas preferences:\n",
    "pd.options.display.max_columns=500\n",
    "pd.options.display.max_colwidth=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load some easy sample data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cancer.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = cancer.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom xgboost classifier class object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, classifier, evaluation_metric, num_rounds, feature_selector=None, validation_size=None):\n",
    "        self.classifier = classifier\n",
    "        self.evaluation_metric = evaluation_metric\n",
    "        self.num_rounds = num_rounds\n",
    "        self.feature_selector = feature_selector\n",
    "        self.validation_size = validation_size\n",
    "        self.classes_ = None\n",
    "        self.classifier_ = None\n",
    "        self.feature_selector_ = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Function for fitting the base estimator.  Implements feature selection and also auto generates a validation set for early stopping.\n",
    "        Also supports probability calibration.\"\"\"\n",
    "        labelbin = LabelBinarizer()\n",
    "        Y = labelbin.fit_transform(y)\n",
    "        self.classes_ = labelbin.classes_\n",
    "        \n",
    "        # Do feature selection if desired\n",
    "        if self.feature_selector:\n",
    "            X_use = self.feature_select(X, y)\n",
    "        else:\n",
    "            X_use = X\n",
    "        \n",
    "        # Make the validation set:\n",
    "        X_train, X_valid, y_train, y_valid = self.make_validation_set(X_use, y)\n",
    "\n",
    "        # Fit the base estimator\n",
    "        self.classifier_ = clone(self.classifier)\n",
    "        self.classifier = self.classifier_.fit(X_train,\n",
    "                                               y_train,\n",
    "                                               eval_metric=self.evaluation_metric,\n",
    "                                               eval_set=[[X_valid, y_valid]],\n",
    "                                               early_stopping_rounds=self.num_rounds)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Makes predictions on input data from trained classifier.  \n",
    "        If feature selection is used, it is applied automatically to the input data.\"\"\"\n",
    "        # The predict function changes in XGBoost when early stopping is performed.\n",
    "        if self.feature_selector:\n",
    "            X_select = self.feature_selector.transform(X)\n",
    "        else:\n",
    "            X_select = X\n",
    "            \n",
    "        # This method should only be called once the model is fit anyway, and since we are always using early stopping, \n",
    "        # we can set ntree_limit by default.\n",
    "        return self.classifier.predict(X_select, ntree_limit=self.classifier.best_ntree_limit)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Makes probability predictions on input data from trained classifier.  If feature selection is used,\n",
    "        it is applied automatically to the input data.  Since this method should only be called after the \n",
    "        classifier is fit, and since we are always using early stopping, the best_ntree_limit should be defined by default.\"\"\"\n",
    "        if self.feature_selector:\n",
    "            X_select = self.feature_selector.transform(X)\n",
    "        else:\n",
    "            X_select = X\n",
    "        \n",
    "        return self.classifier.predict(X_select, ntree_limit=self.classifier.best_ntree_limit)\n",
    "\n",
    "    def feature_select(self, X, y):\n",
    "        \"\"\"Implements feature selection if so desired.\"\"\"\n",
    "        # TODO: Get tree based feature selection working.\n",
    "        self.feature_selector_ = clone(self.feature_selector)\n",
    "        self.feature_selector = self.feature_selector_.fit(X, y)\n",
    "        \n",
    "        return self.feature_selector.transform(X)\n",
    "        \n",
    "    def make_validation_set(self, X, y):\n",
    "        \"\"\"Makes the validation set.\"\"\"\n",
    "        if self.validation_size:\n",
    "            val_size = self.validation_size\n",
    "        else:\n",
    "            val_size = 0.1\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=val_size)\n",
    "        return X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:basic_ml]",
   "language": "python",
   "name": "conda-env-basic_ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
